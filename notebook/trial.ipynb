{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4b5dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- IMPORTS & ENV -----------------------------------------\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_classic.retrievers.contextual_compression import (\n",
    "    ContextualCompressionRetriever,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "qdrant_url = os.environ.get(\"QDRANT_URL\")\n",
    "qdrant_api_key = os.environ.get(\"QDRANT_API_KEY\")\n",
    "cohere_api_key = os.environ.get(\"COHERE_API_KEY\")\n",
    "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "COLLECTION_NAME = \"compliance_docs\"\n",
    "MODEL_NAME = \"openai/gpt-oss-120b\"\n",
    "TOP_K = 6\n",
    "TOP_N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "174c2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PYDANTIC SCHEMA ----------------------------------------\n",
    "class CitedSource(BaseModel):\n",
    "    document_name: str = Field(description=\"Name of the source document\")\n",
    "    page_number: Optional[int] = Field(description=\"Page number in the document\")\n",
    "    section: Optional[str] = Field(\n",
    "        description=\"Section or article reference if mentioned e.g. Section 4.2 \"\n",
    "    )\n",
    "    law_category: Optional[str] = Field(\n",
    "        description=\"Category of the law e.g. Taxation, Data Protection, Corporate/CAC, Employment, General Compliance\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ComplianceAnswer(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"The compliance answer based strictly on the provided context\"\n",
    "    )\n",
    "    found_in_docs: bool = Field(\n",
    "        description=\"True if the answer was found in the documents, False if not\"\n",
    "    )\n",
    "    sources: list[CitedSource] = Field(\n",
    "        description=\"List of sources that support the answer\"\n",
    "    )\n",
    "    confidence: str = Field(\n",
    "        description=\"How confident the answer is: high, medium, or low\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34ff3469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:06<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2018 pages\n",
      "Sample: 2020 No. 3          A 1\n",
      "Companies and Allied Matters Act, 2020\n",
      "Federal Republic of Nigeria\n",
      "Official ...\n"
     ]
    }
   ],
   "source": [
    "# ---- LOAD DOCUMENTS -----------------------------------------\n",
    "loader = DirectoryLoader(\n",
    "    path=\"../documents/\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress=True,\n",
    ")\n",
    "doc = loader.load()\n",
    "print(f\"Loaded {len(doc)} pages\")\n",
    "print(f\"Sample: {doc[0].page_content[:100]}...\")\n",
    "\n",
    "\n",
    "# ---- CLEAN DOCUMENTS ----------------------------------------\n",
    "for d in doc:\n",
    "    d.page_content = re.sub(\n",
    "        r\"[ \\t]+\",\n",
    "        \" \",\n",
    "        re.sub(r\"\\n{3,}\", \"\\n\\n\", re.sub(r\"\\r\\n?\", \"\\n\", d.page_content)),\n",
    "    ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f0b7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2100 chunks\n",
      "Created 2100 tagged chunks\n"
     ]
    }
   ],
   "source": [
    "# ---- CHUNK DOCUMENTS ----------------------------------------\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=900,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\n",
    "        \"\\nPART \",\n",
    "        \"\\nPart \",\n",
    "        \"\\nCHAPTER \",\n",
    "        \"\\nChapter \",\n",
    "        \"\\nSECTION \",\n",
    "        \"\\nSection \",\n",
    "        \"\\nArticle \",\n",
    "        \"\\nARTICLE \",\n",
    "        \"\\nSCHEDULE \",\n",
    "        \"\\nSchedule \",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n(\",\n",
    "        \"\\n(a)\",\n",
    "        \"\\n(i)\",\n",
    "        \"\\n• \",\n",
    "        \"\\n- \",\n",
    "        \"\\n\",\n",
    "        \". \",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ],\n",
    "    add_start_index=True,\n",
    ")\n",
    "chunks = splitter.split_documents(doc)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "\n",
    "# Metadata tagging this helps the AI filter by law type during retrieval\n",
    "for chunk in chunks:\n",
    "    source_name = chunk.metadata.get(\"source\", \"\").lower()\n",
    "\n",
    "    if any(word in source_name for word in [\"tax\", \"nta\", \"revenue\", \"firs\", \"nrs\"]):\n",
    "        chunk.metadata[\"law_category\"] = \"Taxation\"\n",
    "    elif any(word in source_name for word in [\"data\", \"ndpa\", \"privacy\", \"ndpc\"]):\n",
    "        chunk.metadata[\"law_category\"] = \"Data Protection\"\n",
    "    elif any(\n",
    "        word in source_name for word in [\"cama\", \"cac\", \"corporate\", \"annual return\"]\n",
    "    ):\n",
    "        chunk.metadata[\"law_category\"] = \"Corporate/CAC\"\n",
    "    elif any(word in source_name for word in [\"labour\", \"pension\", \"wage\", \"employee\"]):\n",
    "        chunk.metadata[\"law_category\"] = \"Employment\"\n",
    "    else:\n",
    "        chunk.metadata[\"law_category\"] = \"General Compliance\"\n",
    "print(f\"Created {len(chunks)} tagged chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5bbd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting into Qdrant...\n",
      "Ingestion complete\n"
     ]
    }
   ],
   "source": [
    "dense_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\", api_key=openai_api_key\n",
    ")\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "print(\"Ingesting into Qdrant...\")\n",
    "qdrant_store = QdrantVectorStore.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=dense_embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    ")\n",
    "print(\"Ingestion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "673c83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- RETRIEVER + COHERE RERANKER ----------------------------\n",
    "base_retriever = qdrant_store.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "reranker = CohereRerank(\n",
    "    cohere_api_key=cohere_api_key,\n",
    "    model=\"rerank-english-v3.0\",\n",
    "    top_n=TOP_N,\n",
    ")\n",
    "\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c282a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- LLM SETUP + STRUCTURED OUTPUT --------------------------\n",
    "llm = ChatGroq(model=MODEL_NAME, api_key=groq_api_key, temperature=0)\n",
    "\n",
    "# Binds the Pydantic schema to the LLM.\n",
    "structured_llm = llm.with_structured_output(ComplianceAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f531cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SQLITE CHAT HISTORY SETUP ------------------------------\n",
    "\n",
    "conn = sqlite3.connect(\"chat_history.db\")\n",
    "conn.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS messages (\n",
    "        id        INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        session   TEXT    NOT NULL,\n",
    "        role      TEXT    NOT NULL,\n",
    "        content   TEXT    NOT NULL,\n",
    "        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47b83ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 messages from history\n"
     ]
    }
   ],
   "source": [
    "# ---- LOAD HISTORY FROM SQLITE -------------------------------\n",
    "session_id = \"session_1\"\n",
    "conn = sqlite3.connect(\"chat_history.db\")\n",
    "rows = conn.execute(\n",
    "    \"SELECT role, content FROM messages WHERE session = ? ORDER BY id\", (session_id,)\n",
    ").fetchall()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "for role, content in rows:\n",
    "    if role == \"human\":\n",
    "        chat_history.append(HumanMessage(content=content))\n",
    "    else:\n",
    "        chat_history.append(AIMessage(content=content))\n",
    "\n",
    "print(f\"Loaded {len(chat_history)} messages from history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21059f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone question: According to the NDPR 2019, what specific criteria require a Data Controller to file an annual Data Protection Audit, and what is the annual deadline for submitting this audit?\n"
     ]
    }
   ],
   "source": [
    "# ---- USER QUESTION ----------\n",
    "question = \"According to the NDPR 2019, what are the specific criteria that mandate a Data Controller to file an annual Data Protection Audit, and what is the annual deadline for this filing?\"\n",
    "\n",
    "CONTEXTUALIZE_PROMPT = \"\"\"Given the chat history and the latest user question, \\\n",
    "rewrite the question as a fully standalone question that can be understood \\\n",
    "without the chat history. Do NOT answer it, only rewrite it. \\\n",
    "If it is already standalone, return it as is.\"\"\"\n",
    "\n",
    "if chat_history:\n",
    "    contextualize_messages = (\n",
    "        [SystemMessage(content=CONTEXTUALIZE_PROMPT)]\n",
    "        + chat_history\n",
    "        + [HumanMessage(content=question)]\n",
    "    )\n",
    "    standalone_question = llm.invoke(contextualize_messages).content\n",
    "else:\n",
    "    standalone_question = question\n",
    "\n",
    "print(f\"Standalone question: {standalone_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb346c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 chunks after reranking\n"
     ]
    }
   ],
   "source": [
    "# ---- RETRIEVE + RERANK -------\n",
    "retrieved_docs = retriever.invoke(standalone_question)\n",
    "print(f\"Retrieved {len(retrieved_docs)} chunks after reranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "274c75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- FORMAT CONTEXT ----------------------\n",
    "context_parts = []\n",
    "for i, d in enumerate(retrieved_docs, 1):\n",
    "    source = d.metadata.get(\"source\", \"Unknown\")\n",
    "    page = d.metadata.get(\"page\", \"?\")\n",
    "    category = d.metadata.get(\"law_category\", \"General Compliance\")\n",
    "    context_parts.append(\n",
    "        f\"[{i}] Source: {source} | Page: {page} | Category: {category}\\n{d.page_content}\"\n",
    "    )\n",
    "\n",
    "context = \"\\n\\n---\\n\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c28c7a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[1] Source: ..\\\\documents\\\\NDPR Audit Template.pdf | Page: 19 | Category: General Compliance\\n19 | P a g e \\n \\n✓ to assess the level of compliance with the NDPR \\n✓ to evaluate compliance with the organisation's own data protection policy \\n✓ to identify potential gaps and weaknesses in organisation’s processes \\n✓ to give requisite advice and/or remedial actions for identified gaps \\n \\n7.1 Audit Periods \\nArticle 4.1(7) of the Regulation addresses the period when audit report is to be \\nfiled by Data Controllers. The Article provides as follows: \\n(7) On annual basis, a Data Controller who processed the Personal \\nData of more than 2000 Data Subjects in a period of 12 months shall, \\nnot later than the 15th of March of the following year, submit a \\nsummary of its data protection audit to the Agency. The data \\nprotection audit shall contain information as specified in 4.1(5). \\nNon-filing of annual audit report by a Data Controller, as required by NDPR, is a \\nprima facie case of breach. 15th of March is the latest date for filing of Annual \\nData Audit Report. \\n7.2 Audit Filing Fees \\nEach Controller is expected to file the audit report through a DPCO and pay the \\nfollowing amount as applicable: \\nFiling Fees for Annual Audit Reports \\nA) \\nFiling of Report of less than 2,000 Data Subjects \\nN10,000 \\nB) \\nFiling of Report of 2,000 Data Subjects and above \\nN20,000 \\n \\n7.3 Content of Audit Report \\nThe data protection audit shall contain information as specified in Article 4.1(5) \\nof the Regulation. For clarity, the report shall contain the following:\\n\\n---\\n\\n[2] Source: ..\\\\documents\\\\NDPR Audit Template.pdf | Page: 19 | Category: General Compliance\\n19 | P a g e \\n \\n✓ to assess the level of compliance with the NDPR \\n✓ to evaluate compliance with the organisation's own data protection policy \\n✓ to identify potential gaps and weaknesses in organisation’s processes \\n✓ to give requisite advice and/or remedial actions for identified gaps \\n \\n7.1 Audit Periods \\nArticle 4.1(7) of the Regulation addresses the period when audit report is to be \\nfiled by Data Controllers. The Article provides as follows: \\n(7) On annual basis, a Data Controller who processed the Personal \\nData of more than 2000 Data Subjects in a period of 12 months shall, \\nnot later than the 15th of March of the following year, submit a \\nsummary of its data protection audit to the Agency. The data \\nprotection audit shall contain information as specified in 4.1(5). \\nNon-filing of annual audit report by a Data Controller, as required by NDPR, is a \\nprima facie case of breach. 15th of March is the latest date for filing of Annual \\nData Audit Report. \\n7.2 Audit Filing Fees \\nEach Controller is expected to file the audit report through a DPCO and pay the \\nfollowing amount as applicable: \\nFiling Fees for Annual Audit Reports \\nA) \\nFiling of Report of less than 2,000 Data Subjects \\nN10,000 \\nB) \\nFiling of Report of 2,000 Data Subjects and above \\nN20,000 \\n \\n7.3 Content of Audit Report \\nThe data protection audit shall contain information as specified in Article 4.1(5) \\nof the Regulation. For clarity, the report shall contain the following:\\n\\n---\\n\\n[3] Source: ..\\\\documents\\\\NDPR Audit Template.pdf | Page: 19 | Category: General Compliance\\n19 | P a g e \\n \\n✓ to assess the level of compliance with the NDPR \\n✓ to evaluate compliance with the organisation's own data protection policy \\n✓ to identify potential gaps and weaknesses in organisation’s processes \\n✓ to give requisite advice and/or remedial actions for identified gaps \\n \\n7.1 Audit Periods \\nArticle 4.1(7) of the Regulation addresses the period when audit report is to be \\nfiled by Data Controllers. The Article provides as follows: \\n(7) On annual basis, a Data Controller who processed the Personal \\nData of more than 2000 Data Subjects in a period of 12 months shall, \\nnot later than the 15th of March of the following year, submit a \\nsummary of its data protection audit to the Agency. The data \\nprotection audit shall contain information as specified in 4.1(5). \\nNon-filing of annual audit report by a Data Controller, as required by NDPR, is a \\nprima facie case of breach. 15th of March is the latest date for filing of Annual \\nData Audit Report. \\n7.2 Audit Filing Fees \\nEach Controller is expected to file the audit report through a DPCO and pay the \\nfollowing amount as applicable: \\nFiling Fees for Annual Audit Reports \\nA) \\nFiling of Report of less than 2,000 Data Subjects \\nN10,000 \\nB) \\nFiling of Report of 2,000 Data Subjects and above \\nN20,000 \\n \\n7.3 Content of Audit Report \\nThe data protection audit shall contain information as specified in Article 4.1(5) \\nof the Regulation. For clarity, the report shall contain the following:\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa3d60d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: A Data Controller must file an annual Data Protection Audit if, in any 12‑month period, it has processed the personal data of more than 2,000 data subjects. The audit summary must be submitted to the Agency no later than 15 March of the following year.\n",
      "Found in docs: True\n",
      "Confidence: high\n",
      "Sources:\n",
      "  - NDPR Audit Template.pdf | Page 19 | Section 7.1 Audit Periods | Category General Compliance\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a precise compliance assistant. \\\n",
    "Answer using ONLY the provided context from the compliance documents. \\\n",
    "Always cite the source document name, page number, and section if available. \\\n",
    "Set found_in_docs to False and explain that the answer was not found if the context does not support an answer. \\\n",
    "Set confidence based on how clearly the context supports the answer: high, medium, or low.\"\"\"\n",
    "\n",
    "messages = (\n",
    "    [SystemMessage(content=SYSTEM_PROMPT)]\n",
    "    + chat_history\n",
    "    + [HumanMessage(content=f\"Context:\\n{context}\\n\\nQuestion: {question}\")]\n",
    ")\n",
    "\n",
    "result = structured_llm.invoke(messages)\n",
    "\n",
    "print(f\"\\nAnswer: {result.answer}\")\n",
    "print(f\"Found in docs: {result.found_in_docs}\")\n",
    "print(f\"Confidence: {result.confidence}\")\n",
    "print(\"Sources:\")\n",
    "for s in result.sources:\n",
    "    print(\n",
    "        f\"  - {s.document_name} | Page {s.page_number} | Section {s.section} | Category {s.law_category}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0edc8d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved to history.\n"
     ]
    }
   ],
   "source": [
    "# ---- SAVE TO SQLITE -------------\n",
    "conn = sqlite3.connect(\"chat_history.db\")\n",
    "conn.execute(\n",
    "    \"INSERT INTO messages (session, role, content) VALUES (?, ?, ?)\",\n",
    "    (session_id, \"human\", question),\n",
    ")\n",
    "conn.execute(\n",
    "    \"INSERT INTO messages (session, role, content) VALUES (?, ?, ?)\",\n",
    "    (session_id, \"ai\", result.answer),\n",
    ")\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nSaved to history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a3125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
